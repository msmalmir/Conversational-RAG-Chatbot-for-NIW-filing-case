{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d07bcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load the PDF\n",
    "loader = PyPDFLoader(\"Attorney_instruction_case_prepration_NIWI_140.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the document into manageable chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c801146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "Proprietary \n",
      "Confidential \n",
      "Document\n",
      "DOCUMENT LIST: EB-2 NIW\n",
      "• For each of the documents listed, please scan them as PDF files (When scanning the \n",
      "documents, we strongly recommend you use a high-quality scanner rather than your \n",
      "mobile phone to avoid blurring and convert your documents to high-resolution print-\n",
      "ready images.) or provide us electronic/PDF files directly. Then, please highlight \n",
      "your name where appropriate with Adobe Acrobat or Adobe Reader DC.\n",
      "• If you will be filing additional petitions (e.g., EB-1A, EB-1B, O-1), you will need to\n",
      "supply us with a separate set of PDFs of documents for each petition you will be\n",
      "filing.\n",
      "• Some of the documents listed below, particularly those in the “Exhibits” list, may not\n",
      "apply to you. It may not be necessary for your case to have each of the types of\n",
      "evidence listed.\n",
      "• Once your petition letter has been finalized, we will send a follow-up message asking\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 2:\n",
      "apply to you. It may not be necessary for your case to have each of the types of\n",
      "evidence listed.\n",
      "• Once your petition letter has been finalized, we will send a follow-up message asking\n",
      "for a .zip file with all the supporting documents that are listed in the index of exhibits\n",
      "to assemble your package.\n",
      "• Once we have received your evidence for the exhibits, we will conduct reviews of\n",
      "these supporting documents and confirm with you on whether there are files missing.\n",
      "• Once your package is complete with sufficient evidence on our online platform, we\n",
      "will print out all the documents for you from the platform and mail your physical\n",
      "package to the USCIS by FedEx 2-day shipping. Our e-filing process will save you\n",
      "time and shipping/printing fees.\n",
      "• If you have already mailed out the evidence, we will review them after we receive the\n",
      "package. But if you haven’t, please do not mail the package to our office.\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the first few chunks to verify the splitting\n",
    "for i, chunk in enumerate(chunks[:2]):\n",
    "    print(f\"Chunk {i + 1}:\\n{chunk.page_content}\\n{'-'*40}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dcea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize embeddings \n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"  )\n",
    "\n",
    "# Move the embeddings model to the GPU \n",
    "embeddings.client = embeddings.client.to(\"cuda\")  # Move the underlying model to GPU\n",
    "\n",
    "# Create a FAISS vector store from the document chunks\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "\n",
    "# Set up FAISS vector store as a retriever\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b74994",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Choose a  Hugging Face model \n",
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name, torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "# Create a text generation pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", \n",
    "                       model=model, \n",
    "                       tokenizer=tokenizer, \n",
    "                       device=0)\n",
    "\n",
    "# Initialize LangChain's HuggingFacePipeline LLM\n",
    "llm = HuggingFacePipeline(pipeline=qa_pipeline)\n",
    "\n",
    "# Set up memory to retain conversation context\n",
    "memory = ConversationBufferMemory()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b798aea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Set up memory to automatically handle chat history\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Create the conversational retrieval chain, with memory automatically managing chat history\n",
    "chat_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    "   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ff0d22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$700\n"
     ]
    }
   ],
   "source": [
    "# Function to retrieve context and interact with the model\n",
    "def chat_with_bot(question):\n",
    "    # Retrieve the relevant context from the retriever based on the question\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "    retrieved_context = \" \".join([doc.page_content for doc in retrieved_docs])\n",
    "    \n",
    "    # Use the QA pipeline with the dynamically retrieved context\n",
    "    response = qa_pipeline(question=question, context=retrieved_context)\n",
    "    return response['answer']\n",
    "\n",
    "# Example usage\n",
    "question = \"How much does the filing fee cost?\"\n",
    "print(chat_with_bot(question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b3c478d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to provide an authoritative perspective on \n",
      "the value and significance of your research\n"
     ]
    }
   ],
   "source": [
    "# Function to retrieve context and interact with the model\n",
    "def chat_with_bot(question):\n",
    "    # Retrieve the relevant context from the retriever based on the question\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "    retrieved_context = \" \".join([doc.page_content for doc in retrieved_docs])\n",
    "    \n",
    "    # Use the QA pipeline with the dynamically retrieved context\n",
    "    response = qa_pipeline(question=question, context=retrieved_context)\n",
    "    return response['answer']\n",
    "\n",
    "# Example usage\n",
    "question = \"why do I need letters of recommendation?\"\n",
    "print(chat_with_bot(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b34cf69-324a-487b-8a61-d43b44df1978",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two\n"
     ]
    }
   ],
   "source": [
    "# Function to retrieve context and interact with the model\n",
    "def chat_with_bot(question):\n",
    "    # Retrieve the relevant context from the retriever based on the question\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "    retrieved_context = \" \".join([doc.page_content for doc in retrieved_docs])\n",
    "    \n",
    "    # Use the QA pipeline with the dynamically retrieved context\n",
    "    response = qa_pipeline(question=question, context=retrieved_context)\n",
    "    return response['answer']\n",
    "\n",
    "# Example usage\n",
    "question = \"how many type of recomenders are there?\"\n",
    "print(chat_with_bot(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91f61cbe-06ed-487d-a27a-d1e5a2a9aab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“dependent” and “independent.”\n"
     ]
    }
   ],
   "source": [
    "# Function to retrieve context and interact with the model\n",
    "def chat_with_bot(question):\n",
    "    # Retrieve the relevant context from the retriever based on the question\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "    retrieved_context = \" \".join([doc.page_content for doc in retrieved_docs])\n",
    "    \n",
    "    # Use the QA pipeline with the dynamically retrieved context\n",
    "    response = qa_pipeline(question=question, context=retrieved_context)\n",
    "    return response['answer']\n",
    "\n",
    "# Example usage\n",
    "question = \"what are those two?\"\n",
    "print(chat_with_bot(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c90f762-4b68-400c-8937-715d5b7acfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load the PDF\n",
    "loader = PyPDFLoader(\"Attorney_instruction_case_prepration_NIWI_140.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "\n",
    "\n",
    "# Split the document into manageable chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Display the first few chunks to verify the splitting\n",
    "for i, chunk in enumerate(chunks[:2]):\n",
    "    print(f\"Chunk {i + 1}:\\n{chunk.page_content}\\n{'-'*40}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize embeddings \n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"  )\n",
    "\n",
    "# Move the embeddings model to the GPU \n",
    "embeddings.client = embeddings.client.to(\"cuda\")  # Move the underlying model to GPU\n",
    "\n",
    "# Create a FAISS vector store from the document chunks\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "\n",
    "# Set up FAISS vector store as a retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Choose a  Hugging Face model \n",
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name, torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "# Create a text generation pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", \n",
    "                       model=model, \n",
    "                       tokenizer=tokenizer, \n",
    "                       device=0)\n",
    "\n",
    "# Initialize LangChain's HuggingFacePipeline LLM\n",
    "llm = HuggingFacePipeline(pipeline=qa_pipeline)\n",
    "\n",
    "# Set up memory to retain conversation context\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Set up memory to automatically handle chat history\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Create the conversational retrieval chain, with memory automatically managing chat history\n",
    "chat_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    "   \n",
    ")\n",
    "\n",
    "\n",
    "# Function to retrieve context and interact with the model\n",
    "def chat_with_bot(question):\n",
    "    # Retrieve the relevant context from the retriever based on the question\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "    retrieved_context = \" \".join([doc.page_content for doc in retrieved_docs])\n",
    "    \n",
    "    # Use the QA pipeline with the dynamically retrieved context\n",
    "    response = qa_pipeline(question=question, context=retrieved_context)\n",
    "    return response['answer']\n",
    "\n",
    "# Example usage\n",
    "question = \"How much does the filing fee cost?\"\n",
    "print(chat_with_bot(question))\n",
    "\n",
    "# Function to retrieve context and interact with the model\n",
    "def chat_with_bot(question):\n",
    "    # Retrieve the relevant context from the retriever based on the question\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "    retrieved_context = \" \".join([doc.page_content for doc in retrieved_docs])\n",
    "    \n",
    "    # Use the QA pipeline with the dynamically retrieved context\n",
    "    response = qa_pipeline(question=question, context=retrieved_context)\n",
    "    return response['answer']\n",
    "\n",
    "# Example usage\n",
    "question = \"why do I need letters of recommendation?\"\n",
    "print(chat_with_bot(question))\n",
    "\n",
    "\n",
    "# Function to retrieve context and interact with the model\n",
    "def chat_with_bot(question):\n",
    "    # Retrieve the relevant context from the retriever based on the question\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "    retrieved_context = \" \".join([doc.page_content for doc in retrieved_docs])\n",
    "    \n",
    "    # Use the QA pipeline with the dynamically retrieved context\n",
    "    response = qa_pipeline(question=question, context=retrieved_context)\n",
    "    return response['answer']\n",
    "\n",
    "# Example usage\n",
    "question = \"how many type of recomenders are there?\"\n",
    "print(chat_with_bot(question))\n",
    "\n",
    "# Function to retrieve context and interact with the model\n",
    "def chat_with_bot(question):\n",
    "    # Retrieve the relevant context from the retriever based on the question\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "    retrieved_context = \" \".join([doc.page_content for doc in retrieved_docs])\n",
    "    \n",
    "    # Use the QA pipeline with the dynamically retrieved context\n",
    "    response = qa_pipeline(question=question, context=retrieved_context)\n",
    "    return response['answer']\n",
    "\n",
    "# Example usage\n",
    "question = \"what are those two?\"\n",
    "print(chat_with_bot(question))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChatBot",
   "language": "python",
   "name": "chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
